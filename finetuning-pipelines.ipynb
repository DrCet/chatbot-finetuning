{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:51:47.351432Z","iopub.execute_input":"2025-04-30T16:51:47.351667Z","iopub.status.idle":"2025-04-30T16:51:47.365870Z","shell.execute_reply.started":"2025-04-30T16:51:47.351650Z","shell.execute_reply":"2025-04-30T16:51:47.365035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205052e344f9436fa36dc5337e02e734"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T17:53:01.335741Z","iopub.execute_input":"2025-04-30T17:53:01.336569Z","iopub.status.idle":"2025-04-30T17:53:01.341929Z","shell.execute_reply.started":"2025-04-30T17:53:01.336538Z","shell.execute_reply":"2025-04-30T17:53:01.341210Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!rm -r chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T17:53:02.274269Z","iopub.execute_input":"2025-04-30T17:53:02.274824Z","iopub.status.idle":"2025-04-30T17:53:02.760942Z","shell.execute_reply.started":"2025-04-30T17:53:02.274801Z","shell.execute_reply":"2025-04-30T17:53:02.759833Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"!git clone https://github.com/DrCet/chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T17:53:03.672516Z","iopub.execute_input":"2025-04-30T17:53:03.673195Z","iopub.status.idle":"2025-04-30T17:53:04.284234Z","shell.execute_reply.started":"2025-04-30T17:53:03.673167Z","shell.execute_reply":"2025-04-30T17:53:04.283459Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'chatbot-finetuning'...\nremote: Enumerating objects: 177, done.\u001b[K\nremote: Counting objects: 100% (177/177), done.\u001b[K\nremote: Compressing objects: 100% (122/122), done.\u001b[K\nremote: Total 177 (delta 93), reused 128 (delta 48), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (177/177), 44.68 KiB | 6.38 MiB/s, done.\nResolving deltas: 100% (93/93), done.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"cd chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T17:53:05.358164Z","iopub.execute_input":"2025-04-30T17:53:05.358459Z","iopub.status.idle":"2025-04-30T17:53:05.363678Z","shell.execute_reply.started":"2025-04-30T17:53:05.358436Z","shell.execute_reply":"2025-04-30T17:53:05.363087Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/chatbot-finetuning\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# Finetune a chat model","metadata":{}},{"cell_type":"code","source":"!python create_model.py \\\n  --model_name \"google-t5/t5-small\" \\\n  --pytorch_dump_folder_path \"t5-small\" \\\n  --model_type \"seq2seq\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:51:19.837613Z","iopub.execute_input":"2025-04-28T15:51:19.837902Z","iopub.status.idle":"2025-04-28T15:51:54.299937Z","shell.execute_reply.started":"2025-04-28T15:51:19.837868Z","shell.execute_reply":"2025-04-28T15:51:54.299058Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"config.json: 100%|█████████████████████████| 1.21k/1.21k [00:00<00:00, 6.95MB/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n2025-04-28 15:51:33.802187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745855493.985337      73 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745855494.037083      73 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nmodel.safetensors: 100%|██████████████████████| 242M/242M [00:01<00:00, 209MB/s]\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\ngeneration_config.json: 100%|██████████████████| 147/147 [00:00<00:00, 1.10MB/s]\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\ntokenizer_config.json: 100%|███████████████| 2.32k/2.32k [00:00<00:00, 13.3MB/s]\nspiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 4.79MB/s]\ntokenizer.json: 100%|██████████████████████| 1.39M/1.39M [00:00<00:00, 5.19MB/s]\nloading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nConfiguration saved in t5-small/config.json\nConfiguration saved in t5-small/generation_config.json\nModel weights saved in t5-small/model.safetensors\ntokenizer config file saved in t5-small/tokenizer_config.json\nSpecial tokens file saved in t5-small/special_tokens_map.json\nCopy vocab file to t5-small/spiece.model\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!accelerate launch run_chatbot_finetuning.py ./finetune_json/chatbot.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:51:54.301021Z","iopub.execute_input":"2025-04-28T15:51:54.301479Z","iopub.status.idle":"2025-04-28T19:26:49.685490Z","shell.execute_reply.started":"2025-04-28T15:51:54.301447Z","shell.execute_reply":"2025-04-28T19:26:49.684502Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"README.md: 100%|███████████████████████████| 1.06k/1.06k [00:00<00:00, 6.73MB/s]\n(…)-00000-of-00004-2d5a1467fff1081b.parquet: 100%|█| 249M/249M [00:00<00:00, 287\n(…)-00001-of-00004-5852b56a2bd28fd9.parquet: 100%|█| 248M/248M [00:00<00:00, 282\n(…)-00002-of-00004-a26307300439e943.parquet: 100%|█| 246M/246M [00:00<00:00, 308\n(…)-00003-of-00004-d243063613e5a057.parquet: 100%|█| 248M/248M [00:00<00:00, 324\n(…)-00000-of-00001-869c898b519ad725.parquet: 100%|█| 9.99M/9.99M [00:00<00:00, 2\nGenerating train split: 100%|█| 2119719/2119719 [00:06<00:00, 332627.28 examples\nGenerating validation split: 100%|█| 21990/21990 [00:00<00:00, 369827.44 example\n[INFO|configuration_utils.py:691] 2025-04-28 15:52:20,650 >> loading configuration file t5-small/config.json\n[INFO|configuration_utils.py:765] 2025-04-28 15:52:20,653 >> Model config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file spiece.model\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file chat_template.jinja\nPreprocess train dataset (num_proc=4): 100%|█| 2119719/2119719 [20:15<00:00, 174\nPreprocess train dataset (num_proc=4): 100%|█| 21990/21990 [00:12<00:00, 1717.06\n2025-04-28 16:12:51.367361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745856771.389582     100 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745856771.396489     100 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[INFO|modeling_utils.py:1121] 2025-04-28 16:12:53,925 >> loading weights file t5-small/model.safetensors\n[INFO|configuration_utils.py:1142] 2025-04-28 16:12:53,927 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4930] 2025-04-28 16:12:53,983 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4938] 2025-04-28 16:12:53,984 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1095] 2025-04-28 16:12:53,985 >> loading configuration file t5-small/generation_config.json\n[INFO|configuration_utils.py:1142] 2025-04-28 16:12:53,986 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|tokenization_utils_base.py:2510] 2025-04-28 16:12:53,988 >> tokenizer config file saved in ./test_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-28 16:12:53,988 >> Special tokens file saved in ./test_output/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:176] 2025-04-28 16:12:53,989 >> Copy vocab file to ./test_output/spiece.model\n[INFO|configuration_utils.py:419] 2025-04-28 16:12:53,999 >> Configuration saved in ./test_output/config.json\nSteps:   0%|                                             | 0/20 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-04-28 16:12:54,785 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n[WARNING|logging.py:328] 2025-04-28 16:12:55,359 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nSteps:   0%|                    | 0/20 [00:01<?, ?it/s, lr=5e-5, step_loss=2.66]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/chatbot-finetuning/run_chatbot_finetuning.py\", line 807, in <module>\n    main()\n  File \"/kaggle/working/chatbot-finetuning/run_chatbot_finetuning.py\", line 723, in main\n    generated_ids = model.generate(\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2482, in generate\n    result = self._beam_search(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3902, in _beam_search\n    model_outputs = self(**model_inputs, return_dict=True)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n    return model_forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1905, in forward\n    decoder_outputs = self.decoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1131, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 732, in forward\n    hidden_states = self.layer[-1](hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 346, in forward\n    forwarded_states = self.DenseReluDense(forwarded_states)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1732, in _wrapped_call_impl\n    def _wrapped_call_impl(self, *args, **kwargs):\n\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Finetune an imagecap model","metadata":{}},{"cell_type":"code","source":"!python run_imagecap_finetuning.py ./finetune_json/imagecap.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T17:53:08.259525Z","iopub.execute_input":"2025-04-30T17:53:08.259788Z","iopub.status.idle":"2025-04-30T18:01:58.516243Z","shell.execute_reply.started":"2025-04-30T17:53:08.259770Z","shell.execute_reply":"2025-04-30T18:01:58.515468Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2025-04-30 17:53:13.549473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746035593.573025     563 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746035593.580157     563 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 100%|███████| 1340/1340 [00:08<00:00, 157.27 examples/s]\nGenerating validation split: 100%|████| 336/336 [00:02<00:00, 167.96 examples/s]\nconfig.json: 100%|█████████████████████████| 2.82k/2.82k [00:00<00:00, 17.8MB/s]\n[INFO|configuration_utils.py:693] 2025-04-30 17:53:31,398 >> loading configuration file config.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/config.json\n[INFO|configuration_git.py:199] 2025-04-30 17:53:31,399 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:765] 2025-04-30 17:53:31,400 >> Model config GitConfig {\n  \"architectures\": [\n    \"GitForCausalLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 101,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 102,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"git\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"num_image_with_embedding\": null,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vision_config\": {\n    \"attention_dropout\": 0.0,\n    \"dropout\": 0.0,\n    \"hidden_act\": \"quick_gelu\",\n    \"hidden_size\": 768,\n    \"image_size\": 224,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"model_type\": \"git_vision_model\",\n    \"num_attention_heads\": 12,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 12,\n    \"patch_size\": 16,\n    \"projection_dim\": 512\n  },\n  \"vocab_size\": 30522\n}\n\npreprocessor_config.json: 100%|████████████████| 503/503 [00:00<00:00, 5.76MB/s]\n[INFO|image_processing_base.py:380] 2025-04-30 17:53:31,779 >> loading configuration file preprocessor_config.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/preprocessor_config.json\n[INFO|image_processing_base.py:380] 2025-04-30 17:53:31,861 >> loading configuration file preprocessor_config.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/preprocessor_config.json\n[WARNING|logging.py:328] 2025-04-30 17:53:31,861 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|image_processing_base.py:433] 2025-04-30 17:53:31,862 >> Image processor CLIPImageProcessor {\n  \"crop_size\": {\n    \"height\": 224,\n    \"width\": 224\n  },\n  \"do_center_crop\": true,\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"CLIPImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"processor_class\": \"GitProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"shortest_edge\": 224\n  }\n}\n\ntokenizer_config.json: 100%|███████████████████| 453/453 [00:00<00:00, 3.75MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 2.28MB/s]\ntokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 7.70MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 125/125 [00:00<00:00, 1.05MB/s]\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file vocab.txt from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/vocab.txt\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file tokenizer.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file special_tokens_map.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file tokenizer_config.json from cache at ./cache/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-04-30 17:53:33,215 >> loading file chat_template.jinja from cache at None\n[INFO|processing_utils.py:884] 2025-04-30 17:53:33,590 >> Processor GitProcessor:\n- image_processor: CLIPImageProcessor {\n  \"crop_size\": {\n    \"height\": 224,\n    \"width\": 224\n  },\n  \"do_center_crop\": true,\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"CLIPImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"processor_class\": \"GitProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"shortest_edge\": 224\n  }\n}\n\n- tokenizer: BertTokenizerFast(name_or_path='microsoft/git-base', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)\n\n{\n  \"processor_class\": \"GitProcessor\"\n}\n\nPreprocess train dataset (num_proc=4): 100%|█| 1000/1000 [00:44<00:00, 22.65 exa\nPreprocess train dataset (num_proc=4): 100%|█| 200/200 [00:09<00:00, 20.25 examp\n[INFO|configuration_utils.py:693] 2025-04-30 17:54:28,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/config.json\n[INFO|configuration_git.py:199] 2025-04-30 17:54:28,167 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:765] 2025-04-30 17:54:28,168 >> Model config GitConfig {\n  \"architectures\": [\n    \"GitForCausalLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 101,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 102,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"git\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"num_image_with_embedding\": null,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vision_config\": {\n    \"attention_dropout\": 0.0,\n    \"dropout\": 0.0,\n    \"hidden_act\": \"quick_gelu\",\n    \"hidden_size\": 768,\n    \"image_size\": 224,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"model_type\": \"git_vision_model\",\n    \"num_attention_heads\": 12,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 12,\n    \"patch_size\": 16,\n    \"projection_dim\": 512\n  },\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:1124] 2025-04-30 17:54:28,215 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/model.safetensors\n[INFO|configuration_utils.py:1142] 2025-04-30 17:54:28,218 >> Generate config GenerationConfig {\n  \"bos_token_id\": 101,\n  \"eos_token_id\": 102,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4930] 2025-04-30 17:54:28,371 >> All model checkpoint weights were used when initializing GitForCausalLM.\n\n[INFO|modeling_utils.py:4938] 2025-04-30 17:54:28,371 >> All the weights of GitForCausalLM were initialized from the model checkpoint at microsoft/git-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GitForCausalLM for predictions without further training.\n[INFO|configuration_utils.py:1097] 2025-04-30 17:54:28,457 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--git-base/snapshots/1f7fe8444292beb4a259e3a5b6eba440cd5999d4/generation_config.json\n[INFO|configuration_utils.py:1142] 2025-04-30 17:54:28,457 >> Generate config GenerationConfig {\n  \"bos_token_id\": 101,\n  \"eos_token_id\": 102,\n  \"pad_token_id\": 0\n}\n\n[INFO|image_processing_base.py:260] 2025-04-30 17:54:28,458 >> Image processor saved in ./output/preprocessor_config.json\n[INFO|tokenization_utils_base.py:2510] 2025-04-30 17:54:28,458 >> tokenizer config file saved in ./output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-30 17:54:28,459 >> Special tokens file saved in ./output/special_tokens_map.json\n[INFO|configuration_git.py:199] 2025-04-30 17:54:28,502 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 17:54:28,503 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:419] 2025-04-30 17:54:28,504 >> Configuration saved in ./output/config.json\n[INFO|trainer.py:748] 2025-04-30 17:54:29,396 >> Using auto half precision backend\n[INFO|trainer.py:2414] 2025-04-30 17:54:29,739 >> ***** Running training *****\n[INFO|trainer.py:2415] 2025-04-30 17:54:29,739 >>   Num examples = 1,000\n[INFO|trainer.py:2416] 2025-04-30 17:54:29,740 >>   Num Epochs = 3\n[INFO|trainer.py:2417] 2025-04-30 17:54:29,740 >>   Instantaneous batch size per device = 8\n[INFO|trainer.py:2420] 2025-04-30 17:54:29,740 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2421] 2025-04-30 17:54:29,740 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2422] 2025-04-30 17:54:29,740 >>   Total optimization steps = 375\n[INFO|trainer.py:2423] 2025-04-30 17:54:29,741 >>   Number of trainable parameters = 176,619,066\n[INFO|configuration_git.py:199] 2025-04-30 17:54:29,745 >> vision_config is None. initializing the GitVisionConfig with default values.\n  0%|                                                   | 0/375 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-04-30 17:54:30,403 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n{'loss': 5.252, 'grad_norm': 3.171394109725952, 'learning_rate': 3.6933333333333334e-05, 'epoch': 0.8}\n 33%|█████████████▋                           | 125/375 [02:16<03:57,  1.05it/s][INFO|trainer.py:3984] 2025-04-30 17:56:46,230 >> Saving model checkpoint to ./output/checkpoint-125\n[INFO|configuration_git.py:199] 2025-04-30 17:56:46,231 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 17:56:46,231 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 17:56:46,232 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:419] 2025-04-30 17:56:46,233 >> Configuration saved in ./output/checkpoint-125/config.json\n[INFO|configuration_utils.py:911] 2025-04-30 17:56:46,234 >> Configuration saved in ./output/checkpoint-125/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-04-30 17:56:47,949 >> Model weights saved in ./output/checkpoint-125/model.safetensors\n{'loss': 3.9593, 'grad_norm': 4.477934837341309, 'learning_rate': 2.36e-05, 'epoch': 1.6}\n 67%|███████████████████████████▎             | 250/375 [04:38<01:53,  1.10it/s][INFO|trainer.py:3984] 2025-04-30 17:59:08,105 >> Saving model checkpoint to ./output/checkpoint-250\n[INFO|configuration_git.py:199] 2025-04-30 17:59:08,106 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 17:59:08,106 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 17:59:08,107 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:419] 2025-04-30 17:59:08,107 >> Configuration saved in ./output/checkpoint-250/config.json\n[INFO|configuration_utils.py:911] 2025-04-30 17:59:08,108 >> Configuration saved in ./output/checkpoint-250/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-04-30 17:59:09,793 >> Model weights saved in ./output/checkpoint-250/model.safetensors\n{'loss': 3.4334, 'grad_norm': 3.6083481311798096, 'learning_rate': 1.0266666666666668e-05, 'epoch': 2.4}\n100%|█████████████████████████████████████████| 375/375 [07:01<00:00,  1.08it/s][INFO|trainer.py:3984] 2025-04-30 18:01:30,761 >> Saving model checkpoint to ./output/checkpoint-375\n[INFO|configuration_git.py:199] 2025-04-30 18:01:30,761 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 18:01:30,761 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 18:01:30,762 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:419] 2025-04-30 18:01:30,763 >> Configuration saved in ./output/checkpoint-375/config.json\n[INFO|configuration_utils.py:911] 2025-04-30 18:01:30,763 >> Configuration saved in ./output/checkpoint-375/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-04-30 18:01:32,449 >> Model weights saved in ./output/checkpoint-375/model.safetensors\n[INFO|trainer.py:2681] 2025-04-30 18:01:34,505 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 424.7646, 'train_samples_per_second': 7.063, 'train_steps_per_second': 0.883, 'train_loss': 4.0211796468098955, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 375/375 [07:04<00:00,  1.13s/it]\n[INFO|trainer.py:3984] 2025-04-30 18:01:34,508 >> Saving model checkpoint to ./output\n[INFO|configuration_git.py:199] 2025-04-30 18:01:34,508 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 18:01:34,509 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_git.py:199] 2025-04-30 18:01:34,509 >> vision_config is None. initializing the GitVisionConfig with default values.\n[INFO|configuration_utils.py:419] 2025-04-30 18:01:34,510 >> Configuration saved in ./output/config.json\n[INFO|configuration_utils.py:911] 2025-04-30 18:01:34,511 >> Configuration saved in ./output/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-04-30 18:01:36,258 >> Model weights saved in ./output/model.safetensors\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =   548224GF\n  train_loss               =     4.0212\n  train_runtime            = 0:07:04.76\n  train_samples_per_second =      7.063\n  train_steps_per_second   =      0.883\n[INFO|trainer.py:4307] 2025-04-30 18:01:36,266 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4309] 2025-04-30 18:01:36,266 >>   Num examples = 200\n[INFO|trainer.py:4312] 2025-04-30 18:01:36,266 >>   Batch size = 8\n100%|███████████████████████████████████████████| 25/25 [00:18<00:00,  1.34it/s]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_loss               =     3.3975\n  eval_runtime            = 0:00:20.00\n  eval_samples_per_second =      9.997\n  eval_steps_per_second   =       1.25\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}