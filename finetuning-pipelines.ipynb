{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:51:47.351432Z","iopub.execute_input":"2025-04-30T16:51:47.351667Z","iopub.status.idle":"2025-04-30T16:51:47.365870Z","shell.execute_reply.started":"2025-04-30T16:51:47.351650Z","shell.execute_reply":"2025-04-30T16:51:47.365035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205052e344f9436fa36dc5337e02e734"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:05.121032Z","iopub.execute_input":"2025-05-04T14:53:05.121882Z","iopub.status.idle":"2025-05-04T14:53:05.127408Z","shell.execute_reply.started":"2025-05-04T14:53:05.121848Z","shell.execute_reply":"2025-05-04T14:53:05.126493Z"}},"outputs":[{"name":"stdout","text":"/kaggle\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"!rm -r chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:05.791196Z","iopub.execute_input":"2025-05-04T14:53:05.791753Z","iopub.status.idle":"2025-05-04T14:53:06.313678Z","shell.execute_reply.started":"2025-05-04T14:53:05.791725Z","shell.execute_reply":"2025-05-04T14:53:06.312676Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"!git clone https://github.com/DrCet/chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:06.925431Z","iopub.execute_input":"2025-05-04T14:53:06.926275Z","iopub.status.idle":"2025-05-04T14:53:07.480838Z","shell.execute_reply.started":"2025-05-04T14:53:06.926242Z","shell.execute_reply":"2025-05-04T14:53:07.480084Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'chatbot-finetuning'...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"remote: Enumerating objects: 368, done.\u001b[K\nremote: Counting objects: 100% (10/10), done.\u001b[K\nremote: Compressing objects: 100% (9/9), done.\u001b[K\nremote: Total 368 (delta 3), reused 6 (delta 1), pack-reused 358 (from 1)\u001b[K\nReceiving objects: 100% (368/368), 75.85 KiB | 2.45 MiB/s, done.\nResolving deltas: 100% (221/221), done.\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"cd chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:08.067555Z","iopub.execute_input":"2025-05-04T14:53:08.068241Z","iopub.status.idle":"2025-05-04T14:53:08.073691Z","shell.execute_reply.started":"2025-05-04T14:53:08.068208Z","shell.execute_reply":"2025-05-04T14:53:08.073071Z"}},"outputs":[{"name":"stdout","text":"/kaggle/chatbot-finetuning\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"# Finetune a text classification model","metadata":{}},{"cell_type":"code","source":"!python run_seqclass_finetuning.py ./finetune_json/seqclass.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:51:36.505318Z","iopub.execute_input":"2025-05-03T15:51:36.505580Z","iopub.status.idle":"2025-05-03T15:53:59.658413Z","shell.execute_reply.started":"2025-05-03T15:51:36.505562Z","shell.execute_reply":"2025-05-03T15:53:59.657495Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2025-05-03 15:51:42.073786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746287502.097674     837 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746287502.104529     837 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 100%|████| 4205/4205 [00:00<00:00, 174358.39 examples/s]\nconfig.json: 100%|█████████████████████████████| 795/795 [00:00<00:00, 8.96MB/s]\n[INFO|configuration_utils.py:693] 2025-05-03 15:51:46,180 >> loading configuration file config.json from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/config.json\n[INFO|configuration_utils.py:765] 2025-05-03 15:51:46,181 >> Model config XLMRobertaConfig {\n  \"architectures\": [\n    \"XLMRobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 8194,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\ntokenizer_config.json: 100%|███████████████| 1.17k/1.17k [00:00<00:00, 6.15MB/s]\nsentencepiece.bpe.model: 100%|█████████████| 5.07M/5.07M [00:00<00:00, 58.3MB/s]\ntokenizer.json: 100%|███████████████████████| 17.1M/17.1M [00:00<00:00, 182MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 964/964 [00:00<00:00, 8.48MB/s]\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,975 >> loading file sentencepiece.bpe.model from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/sentencepiece.bpe.model\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,976 >> loading file tokenizer.json from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,976 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,976 >> loading file special_tokens_map.json from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,976 >> loading file tokenizer_config.json from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-03 15:51:46,976 >> loading file chat_template.jinja from cache at None\nPreprocess train dataset (num_proc=4): 100%|█| 3574/3574 [00:02<00:00, 1380.67 e\nPreprocess train dataset (num_proc=4): 100%|█| 631/631 [00:01<00:00, 363.83 exam\nmodel.safetensors: 100%|████████████████████| 2.27G/2.27G [00:08<00:00, 253MB/s]\n[INFO|modeling_utils.py:1124] 2025-05-03 15:52:03,296 >> loading weights file model.safetensors from cache at ./bge-reranker-v2-m3-cache/models--BAAI--bge-reranker-v2-m3/snapshots/953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e/model.safetensors\n[INFO|modeling_utils.py:4920] 2025-05-03 15:52:03,534 >> Some weights of the model checkpoint at BAAI/bge-reranker-v2-m3 were not used when initializing XLMRobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:4932] 2025-05-03 15:52:03,534 >> Some weights of XLMRobertaModel were not initialized from the model checkpoint at BAAI/bge-reranker-v2-m3 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[INFO|configuration_utils.py:419] 2025-05-03 15:52:03,551 >> Configuration saved in trainer_output/config.json\n/kaggle/working/chatbot-finetuning/run_seqclass_finetuning.py:320: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n[INFO|trainer.py:698] 2025-05-03 15:52:04,564 >> max_steps is given, it will override any value given in num_train_epochs\n[INFO|trainer.py:2414] 2025-05-03 15:52:04,924 >> ***** Running training *****\n[INFO|trainer.py:2415] 2025-05-03 15:52:04,924 >>   Num examples = 3,574\n[INFO|trainer.py:2416] 2025-05-03 15:52:04,924 >>   Num Epochs = 1\n[INFO|trainer.py:2417] 2025-05-03 15:52:04,924 >>   Instantaneous batch size per device = 2\n[INFO|trainer.py:2419] 2025-05-03 15:52:04,924 >>   Training with DataParallel so batch size has been adjusted to: 4\n[INFO|trainer.py:2420] 2025-05-03 15:52:04,924 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2421] 2025-05-03 15:52:04,924 >>   Gradient Accumulation steps = 2\n[INFO|trainer.py:2422] 2025-05-03 15:52:04,924 >>   Total optimization steps = 100\n[INFO|trainer.py:2423] 2025-05-03 15:52:04,926 >>   Number of trainable parameters = 568,321,618\n  0%|                                                   | 0/100 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-05-03 15:52:04,935 >> You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 8.8719, 'grad_norm': 11.030061721801758, 'learning_rate': 4.5e-05, 'epoch': 0.02}\n{'loss': 8.7141, 'grad_norm': 9.789928436279297, 'learning_rate': 4.877641290737884e-05, 'epoch': 0.04}\n{'loss': 8.4915, 'grad_norm': 19.275259017944336, 'learning_rate': 4.4700268840168045e-05, 'epoch': 0.07}\n{'loss': 8.1968, 'grad_norm': 36.669132232666016, 'learning_rate': 3.824798160583012e-05, 'epoch': 0.09}\n{'loss': 8.4315, 'grad_norm': 24.2218074798584, 'learning_rate': 3.0197792270443982e-05, 'epoch': 0.11}\n{'loss': 8.1873, 'grad_norm': 21.779203414916992, 'learning_rate': 2.1520672475998373e-05, 'epoch': 0.13}\n{'loss': 7.4203, 'grad_norm': 22.523696899414062, 'learning_rate': 1.3263210930352737e-05, 'epoch': 0.16}\n{'loss': 8.2385, 'grad_norm': 21.140602111816406, 'learning_rate': 6.421379363065142e-06, 'epoch': 0.18}\n{'loss': 7.7388, 'grad_norm': 19.071231842041016, 'learning_rate': 1.8204036358303173e-06, 'epoch': 0.2}\n{'loss': 8.261, 'grad_norm': 23.207998275756836, 'learning_rate': 1.522932452260595e-08, 'epoch': 0.22}\n100%|█████████████████████████████████████████| 100/100 [00:46<00:00,  2.09it/s][INFO|trainer.py:3984] 2025-05-03 15:52:51,365 >> Saving model checkpoint to trainer_output/checkpoint-100\n[INFO|trainer.py:3998] 2025-05-03 15:52:51,367 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2510] 2025-05-03 15:52:56,800 >> tokenizer config file saved in trainer_output/checkpoint-100/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-03 15:52:56,800 >> Special tokens file saved in trainer_output/checkpoint-100/special_tokens_map.json\n[INFO|trainer.py:2681] 2025-05-03 15:53:04,955 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 60.0294, 'train_samples_per_second': 13.327, 'train_steps_per_second': 1.666, 'train_loss': 8.255193405151367, 'epoch': 0.22}\n100%|█████████████████████████████████████████| 100/100 [01:00<00:00,  1.67it/s]\n[INFO|trainer.py:3984] 2025-05-03 15:53:04,958 >> Saving model checkpoint to trainer_output\n[INFO|trainer.py:3998] 2025-05-03 15:53:04,961 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2510] 2025-05-03 15:53:14,571 >> tokenizer config file saved in trainer_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-03 15:53:14,572 >> Special tokens file saved in trainer_output/special_tokens_map.json\n***** train metrics *****\n  epoch                    =     0.2237\n  total_flos               =        0GF\n  train_loss               =     8.2552\n  train_runtime            = 0:01:00.02\n  train_samples_per_second =     13.327\n  train_steps_per_second   =      1.666\n[INFO|trainer.py:4307] 2025-05-03 15:53:14,705 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4309] 2025-05-03 15:53:14,705 >>   Num examples = 631\n[INFO|trainer.py:4312] 2025-05-03 15:53:14,705 >>   Batch size = 4\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|█████████████████████████████████████████| 158/158 [00:41<00:00,  3.79it/s]\n***** eval metrics *****\n  epoch                   =     0.2237\n  eval_loss               =     3.9962\n  eval_runtime            = 0:00:42.34\n  eval_samples_per_second =     14.901\n  eval_steps_per_second   =      3.731\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Finetune a chat model","metadata":{}},{"cell_type":"code","source":"!python create_model.py \\\n  --model_name \"google-t5/t5-small\" \\\n  --pytorch_dump_folder_path \"t5-small\" \\\n  --model_type \"seq2seq\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:51:19.837613Z","iopub.execute_input":"2025-04-28T15:51:19.837902Z","iopub.status.idle":"2025-04-28T15:51:54.299937Z","shell.execute_reply.started":"2025-04-28T15:51:19.837868Z","shell.execute_reply":"2025-04-28T15:51:54.299058Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"config.json: 100%|█████████████████████████| 1.21k/1.21k [00:00<00:00, 6.95MB/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n2025-04-28 15:51:33.802187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745855493.985337      73 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745855494.037083      73 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nmodel.safetensors: 100%|██████████████████████| 242M/242M [00:01<00:00, 209MB/s]\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\ngeneration_config.json: 100%|██████████████████| 147/147 [00:00<00:00, 1.10MB/s]\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\ntokenizer_config.json: 100%|███████████████| 2.32k/2.32k [00:00<00:00, 13.3MB/s]\nspiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 4.79MB/s]\ntokenizer.json: 100%|██████████████████████| 1.39M/1.39M [00:00<00:00, 5.19MB/s]\nloading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nConfiguration saved in t5-small/config.json\nConfiguration saved in t5-small/generation_config.json\nModel weights saved in t5-small/model.safetensors\ntokenizer config file saved in t5-small/tokenizer_config.json\nSpecial tokens file saved in t5-small/special_tokens_map.json\nCopy vocab file to t5-small/spiece.model\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!accelerate launch run_chatbot_finetuning.py ./finetune_json/chatbot.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:51:54.301021Z","iopub.execute_input":"2025-04-28T15:51:54.301479Z","iopub.status.idle":"2025-04-28T19:26:49.685490Z","shell.execute_reply.started":"2025-04-28T15:51:54.301447Z","shell.execute_reply":"2025-04-28T19:26:49.684502Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"README.md: 100%|███████████████████████████| 1.06k/1.06k [00:00<00:00, 6.73MB/s]\n(…)-00000-of-00004-2d5a1467fff1081b.parquet: 100%|█| 249M/249M [00:00<00:00, 287\n(…)-00001-of-00004-5852b56a2bd28fd9.parquet: 100%|█| 248M/248M [00:00<00:00, 282\n(…)-00002-of-00004-a26307300439e943.parquet: 100%|█| 246M/246M [00:00<00:00, 308\n(…)-00003-of-00004-d243063613e5a057.parquet: 100%|█| 248M/248M [00:00<00:00, 324\n(…)-00000-of-00001-869c898b519ad725.parquet: 100%|█| 9.99M/9.99M [00:00<00:00, 2\nGenerating train split: 100%|█| 2119719/2119719 [00:06<00:00, 332627.28 examples\nGenerating validation split: 100%|█| 21990/21990 [00:00<00:00, 369827.44 example\n[INFO|configuration_utils.py:691] 2025-04-28 15:52:20,650 >> loading configuration file t5-small/config.json\n[INFO|configuration_utils.py:765] 2025-04-28 15:52:20,653 >> Model config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file spiece.model\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-28 15:52:20,658 >> loading file chat_template.jinja\nPreprocess train dataset (num_proc=4): 100%|█| 2119719/2119719 [20:15<00:00, 174\nPreprocess train dataset (num_proc=4): 100%|█| 21990/21990 [00:12<00:00, 1717.06\n2025-04-28 16:12:51.367361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745856771.389582     100 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745856771.396489     100 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[INFO|modeling_utils.py:1121] 2025-04-28 16:12:53,925 >> loading weights file t5-small/model.safetensors\n[INFO|configuration_utils.py:1142] 2025-04-28 16:12:53,927 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4930] 2025-04-28 16:12:53,983 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4938] 2025-04-28 16:12:53,984 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1095] 2025-04-28 16:12:53,985 >> loading configuration file t5-small/generation_config.json\n[INFO|configuration_utils.py:1142] 2025-04-28 16:12:53,986 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|tokenization_utils_base.py:2510] 2025-04-28 16:12:53,988 >> tokenizer config file saved in ./test_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-28 16:12:53,988 >> Special tokens file saved in ./test_output/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:176] 2025-04-28 16:12:53,989 >> Copy vocab file to ./test_output/spiece.model\n[INFO|configuration_utils.py:419] 2025-04-28 16:12:53,999 >> Configuration saved in ./test_output/config.json\nSteps:   0%|                                             | 0/20 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-04-28 16:12:54,785 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n[WARNING|logging.py:328] 2025-04-28 16:12:55,359 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nSteps:   0%|                    | 0/20 [00:01<?, ?it/s, lr=5e-5, step_loss=2.66]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/chatbot-finetuning/run_chatbot_finetuning.py\", line 807, in <module>\n    main()\n  File \"/kaggle/working/chatbot-finetuning/run_chatbot_finetuning.py\", line 723, in main\n    generated_ids = model.generate(\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2482, in generate\n    result = self._beam_search(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3902, in _beam_search\n    model_outputs = self(**model_inputs, return_dict=True)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n    return model_forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1905, in forward\n    decoder_outputs = self.decoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1131, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 732, in forward\n    hidden_states = self.layer[-1](hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 346, in forward\n    forwarded_states = self.DenseReluDense(forwarded_states)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1732, in _wrapped_call_impl\n    def _wrapped_call_impl(self, *args, **kwargs):\n\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Finetune an imagecap model","metadata":{}},{"cell_type":"code","source":"!python run_imagecap_finetuning.py ./finetune_json/imagecap.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:11.583007Z","iopub.execute_input":"2025-05-04T14:53:11.583712Z","iopub.status.idle":"2025-05-04T14:56:17.431635Z","shell.execute_reply.started":"2025-05-04T14:53:11.583686Z","shell.execute_reply":"2025-05-04T14:56:17.430400Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-05-04 14:53:16.909885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746370396.932521    1378 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746370396.940374    1378 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 100%|████| 13896/13896 [00:07<00:00, 1893.39 examples/s]\nconfig.json: 100%|█████████████████████████| 4.56k/4.56k [00:00<00:00, 27.7MB/s]\n[INFO|configuration_utils.py:693] 2025-05-04 14:53:29,557 >> loading configuration file config.json from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/config.json\n[INFO|configuration_blip.py:298] 2025-05-04 14:53:29,559 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:53:29,559 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:765] 2025-05-04 14:53:29,560 >> Model config BlipConfig {\n  \"architectures\": [\n    \"BlipForConditionalGeneration\"\n  ],\n  \"image_text_hidden_size\": 256,\n  \"initializer_factor\": 1.0,\n  \"initializer_range\": 0.02,\n  \"label_smoothing\": 0.0,\n  \"logit_scale_init_value\": 2.6592,\n  \"model_type\": \"blip\",\n  \"projection_dim\": 512,\n  \"text_config\": {\n    \"attention_probs_dropout_prob\": 0.0,\n    \"encoder_hidden_size\": 768,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.0,\n    \"hidden_size\": 768,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"label_smoothing\": 0.0,\n    \"layer_norm_eps\": 1e-12,\n    \"max_position_embeddings\": 512,\n    \"model_type\": \"blip_text_model\",\n    \"num_attention_heads\": 12,\n    \"num_hidden_layers\": 12,\n    \"projection_dim\": 768,\n    \"use_cache\": true,\n    \"vocab_size\": 30524\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"vision_config\": {\n    \"attention_dropout\": 0.0,\n    \"dropout\": 0.0,\n    \"hidden_act\": \"gelu\",\n    \"hidden_size\": 768,\n    \"image_size\": 384,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"model_type\": \"blip_vision_model\",\n    \"num_attention_heads\": 12,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 12,\n    \"patch_size\": 16,\n    \"projection_dim\": 512\n  }\n}\n\npreprocessor_config.json: 100%|████████████████| 287/287 [00:00<00:00, 3.38MB/s]\n[INFO|image_processing_base.py:380] 2025-05-04 14:53:29,636 >> loading configuration file preprocessor_config.json from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/preprocessor_config.json\n[WARNING|logging.py:328] 2025-05-04 14:53:29,636 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[INFO|image_processing_utils.py:239] 2025-05-04 14:53:29,637 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 384. Converted to {'height': 384, 'width': 384}.\n[INFO|image_processing_base.py:433] 2025-05-04 14:53:29,637 >> Image processor BlipImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"BlipImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"processor_class\": \"BlipProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 384,\n    \"width\": 384\n  }\n}\n\ntokenizer_config.json: 100%|███████████████████| 506/506 [00:00<00:00, 5.93MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 7.65MB/s]\ntokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 30.7MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 125/125 [00:00<00:00, 1.07MB/s]\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,095 >> loading file vocab.txt from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/vocab.txt\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,096 >> loading file tokenizer.json from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/tokenizer.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,096 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,096 >> loading file special_tokens_map.json from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,096 >> loading file tokenizer_config.json from cache at ./blip-image-captioning-base-cache/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2060] 2025-05-04 14:53:30,096 >> loading file chat_template.jinja from cache at None\nPreprocess train dataset (num_proc=4): 100%|█| 100/100 [00:03<00:00, 30.19 examp\nPreprocess train dataset (num_proc=4): 100%|█| 100/100 [00:03<00:00, 28.04 examp\n[INFO|configuration_utils.py:693] 2025-05-04 14:53:37,410 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/config.json\n[INFO|configuration_blip.py:298] 2025-05-04 14:53:37,411 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:53:37,411 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:765] 2025-05-04 14:53:37,413 >> Model config BlipConfig {\n  \"architectures\": [\n    \"BlipForConditionalGeneration\"\n  ],\n  \"image_text_hidden_size\": 256,\n  \"initializer_factor\": 1.0,\n  \"initializer_range\": 0.02,\n  \"label_smoothing\": 0.0,\n  \"logit_scale_init_value\": 2.6592,\n  \"model_type\": \"blip\",\n  \"projection_dim\": 512,\n  \"text_config\": {\n    \"attention_probs_dropout_prob\": 0.0,\n    \"encoder_hidden_size\": 768,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.0,\n    \"hidden_size\": 768,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"label_smoothing\": 0.0,\n    \"layer_norm_eps\": 1e-12,\n    \"max_position_embeddings\": 512,\n    \"model_type\": \"blip_text_model\",\n    \"num_attention_heads\": 12,\n    \"num_hidden_layers\": 12,\n    \"projection_dim\": 768,\n    \"use_cache\": true,\n    \"vocab_size\": 30524\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"vision_config\": {\n    \"attention_dropout\": 0.0,\n    \"dropout\": 0.0,\n    \"hidden_act\": \"gelu\",\n    \"hidden_size\": 768,\n    \"image_size\": 384,\n    \"initializer_factor\": 1.0,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"model_type\": \"blip_vision_model\",\n    \"num_attention_heads\": 12,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 12,\n    \"patch_size\": 16,\n    \"projection_dim\": 512\n  }\n}\n\n[INFO|modeling_utils.py:1124] 2025-05-04 14:53:37,478 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Salesforce--blip-image-captioning-base/snapshots/82a37760796d32b1411fe092ab5d4e227313294b/pytorch_model.bin\n[INFO|configuration_utils.py:1142] 2025-05-04 14:53:37,481 >> Generate config GenerationConfig {}\n\n[INFO|configuration_utils.py:1142] 2025-05-04 14:53:37,497 >> Generate config GenerationConfig {\n  \"bos_token_id\": 30522,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 0\n}\n\n[INFO|safetensors_conversion.py:61] 2025-05-04 14:53:37,549 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-05-04 14:53:37,890 >> Safetensors PR exists\n[INFO|modeling_utils.py:4930] 2025-05-04 14:53:37,963 >> All model checkpoint weights were used when initializing BlipForConditionalGeneration.\n\n[INFO|modeling_utils.py:4938] 2025-05-04 14:53:37,963 >> All the weights of BlipForConditionalGeneration were initialized from the model checkpoint at Salesforce/blip-image-captioning-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BlipForConditionalGeneration for predictions without further training.\n[INFO|modeling_utils.py:4443] 2025-05-04 14:53:38,004 >> Generation config file not found, using a generation config created from the model config.\n[INFO|image_processing_base.py:260] 2025-05-04 14:53:38,005 >> Image processor saved in trainer_output/preprocessor_config.json\n[INFO|tokenization_utils_base.py:2510] 2025-05-04 14:53:38,006 >> tokenizer config file saved in trainer_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-04 14:53:38,006 >> Special tokens file saved in trainer_output/special_tokens_map.json\n[INFO|configuration_blip.py:298] 2025-05-04 14:53:38,018 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:53:38,018 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:53:38,019 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:53:38,019 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:419] 2025-05-04 14:53:38,020 >> Configuration saved in trainer_output/config.json\n[INFO|trainer.py:2414] 2025-05-04 14:53:39,058 >> ***** Running training *****\n[INFO|trainer.py:2415] 2025-05-04 14:53:39,059 >>   Num examples = 100\n[INFO|trainer.py:2416] 2025-05-04 14:53:39,059 >>   Num Epochs = 3\n[INFO|trainer.py:2417] 2025-05-04 14:53:39,059 >>   Instantaneous batch size per device = 8\n[INFO|trainer.py:2419] 2025-05-04 14:53:39,059 >>   Training with DataParallel so batch size has been adjusted to: 16\n[INFO|trainer.py:2420] 2025-05-04 14:53:39,059 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n[INFO|trainer.py:2421] 2025-05-04 14:53:39,059 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2422] 2025-05-04 14:53:39,059 >>   Total optimization steps = 21\n[INFO|trainer.py:2423] 2025-05-04 14:53:39,060 >>   Number of trainable parameters = 247,414,076\n[INFO|configuration_blip.py:298] 2025-05-04 14:53:39,065 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:53:39,067 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n  0%|                                                    | 0/21 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-05-04 14:53:42,896 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 2.9589, 'grad_norm': 6.195204257965088, 'learning_rate': 4.058724504646834e-05, 'epoch': 1.0}\n 33%|██████████████▋                             | 7/21 [00:36<00:47,  3.38s/it][INFO|trainer.py:3984] 2025-05-04 14:54:15,398 >> Saving model checkpoint to trainer_output/checkpoint-7\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:15,399 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:15,399 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:15,399 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:15,399 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:15,400 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:15,400 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:419] 2025-05-04 14:54:15,401 >> Configuration saved in trainer_output/checkpoint-7/config.json\n[INFO|configuration_utils.py:911] 2025-05-04 14:54:15,401 >> Configuration saved in trainer_output/checkpoint-7/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-05-04 14:54:17,651 >> Model weights saved in trainer_output/checkpoint-7/model.safetensors\n[INFO|trainer.py:4017] 2025-05-04 14:54:17,652 >> Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n[INFO|tokenization_utils_base.py:2510] 2025-05-04 14:54:17,652 >> tokenizer config file saved in trainer_output/checkpoint-7/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-04 14:54:17,653 >> Special tokens file saved in trainer_output/checkpoint-7/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 1.6983, 'grad_norm': 6.423189640045166, 'learning_rate': 1.5866474390840125e-05, 'epoch': 2.0}\n 67%|████████████████████████████▋              | 14/21 [01:17<00:25,  3.63s/it][INFO|trainer.py:3984] 2025-05-04 14:54:56,641 >> Saving model checkpoint to trainer_output/checkpoint-14\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:56,641 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:56,641 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:56,642 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:56,642 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:54:56,643 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:54:56,643 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:419] 2025-05-04 14:54:56,644 >> Configuration saved in trainer_output/checkpoint-14/config.json\n[INFO|configuration_utils.py:911] 2025-05-04 14:54:56,644 >> Configuration saved in trainer_output/checkpoint-14/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-05-04 14:54:58,812 >> Model weights saved in trainer_output/checkpoint-14/model.safetensors\n[INFO|trainer.py:4017] 2025-05-04 14:54:58,813 >> Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n[INFO|tokenization_utils_base.py:2510] 2025-05-04 14:54:58,813 >> tokenizer config file saved in trainer_output/checkpoint-14/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-04 14:54:58,814 >> Special tokens file saved in trainer_output/checkpoint-14/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 1.3675, 'grad_norm': 4.931799411773682, 'learning_rate': 2.7922934437178695e-07, 'epoch': 3.0}\n100%|███████████████████████████████████████████| 21/21 [01:58<00:00,  3.57s/it][INFO|trainer.py:3984] 2025-05-04 14:55:37,206 >> Saving model checkpoint to trainer_output/checkpoint-21\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:37,206 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:37,206 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:37,206 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:37,207 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:37,207 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:37,207 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:419] 2025-05-04 14:55:37,209 >> Configuration saved in trainer_output/checkpoint-21/config.json\n[INFO|configuration_utils.py:911] 2025-05-04 14:55:37,209 >> Configuration saved in trainer_output/checkpoint-21/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-05-04 14:55:39,366 >> Model weights saved in trainer_output/checkpoint-21/model.safetensors\n[INFO|trainer.py:4017] 2025-05-04 14:55:39,366 >> Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n[INFO|tokenization_utils_base.py:2510] 2025-05-04 14:55:39,367 >> tokenizer config file saved in trainer_output/checkpoint-21/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-04 14:55:39,367 >> Special tokens file saved in trainer_output/checkpoint-21/special_tokens_map.json\n[INFO|trainer.py:2681] 2025-05-04 14:55:42,349 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 123.2892, 'train_samples_per_second': 2.433, 'train_steps_per_second': 0.17, 'train_loss': 2.008210772559756, 'epoch': 3.0}\n100%|███████████████████████████████████████████| 21/21 [02:03<00:00,  5.87s/it]\n[INFO|trainer.py:3984] 2025-05-04 14:55:42,352 >> Saving model checkpoint to trainer_output\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:42,353 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:42,353 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:42,353 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:42,353 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_blip.py:298] 2025-05-04 14:55:42,355 >> `text_config` is `None`. Initializing the `BlipTextConfig` with default values.\n[INFO|configuration_blip.py:302] 2025-05-04 14:55:42,355 >> `vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\n[INFO|configuration_utils.py:419] 2025-05-04 14:55:42,360 >> Configuration saved in trainer_output/config.json\n[INFO|configuration_utils.py:911] 2025-05-04 14:55:42,361 >> Configuration saved in trainer_output/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-05-04 14:55:44,890 >> Model weights saved in trainer_output/model.safetensors\n[INFO|trainer.py:4017] 2025-05-04 14:55:44,891 >> Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n[INFO|tokenization_utils_base.py:2510] 2025-05-04 14:55:44,895 >> tokenizer config file saved in trainer_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-05-04 14:55:44,895 >> Special tokens file saved in trainer_output/special_tokens_map.json\n***** train metrics *****\n  epoch                    =         3.0\n  total_flos               = 165800653GF\n  train_loss               =      2.0082\n  train_runtime            =  0:02:03.28\n  train_samples_per_second =       2.433\n  train_steps_per_second   =        0.17\n[INFO|trainer.py:4307] 2025-05-04 14:55:44,945 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4309] 2025-05-04 14:55:44,945 >>   Num examples = 100\n[INFO|trainer.py:4312] 2025-05-04 14:55:44,945 >>   Batch size = 16\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 7/7 [00:20<00:00,  2.94s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_loss               =       1.51\n  eval_runtime            = 0:00:29.99\n  eval_samples_per_second =      3.334\n  eval_steps_per_second   =      0.233\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}