{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -r chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:03:03.811245Z","iopub.execute_input":"2025-04-27T11:03:03.811509Z","iopub.status.idle":"2025-04-27T11:03:03.992597Z","shell.execute_reply.started":"2025-04-27T11:03:03.811488Z","shell.execute_reply":"2025-04-27T11:03:03.991726Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!git clone https://github.com/DrCet/chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:34:52.976840Z","iopub.execute_input":"2025-04-27T13:34:52.977502Z","iopub.status.idle":"2025-04-27T13:34:54.216678Z","shell.execute_reply.started":"2025-04-27T13:34:52.977475Z","shell.execute_reply":"2025-04-27T13:34:54.215988Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'chatbot-finetuning'...\nremote: Enumerating objects: 96, done.\u001b[K\nremote: Counting objects: 100% (96/96), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 96 (delta 47), reused 67 (delta 21), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (96/96), 27.98 KiB | 4.66 MiB/s, done.\nResolving deltas: 100% (47/47), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:05:59.921795Z","iopub.execute_input":"2025-04-27T11:05:59.922093Z","iopub.status.idle":"2025-04-27T11:05:59.938217Z","shell.execute_reply.started":"2025-04-27T11:05:59.922068Z","shell.execute_reply":"2025-04-27T11:05:59.937405Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66c3eb3e72242c2bd18e8c355fb4de2"}},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"cd chatbot-finetuning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:35:05.557145Z","iopub.execute_input":"2025-04-27T13:35:05.557778Z","iopub.status.idle":"2025-04-27T13:35:05.563483Z","shell.execute_reply.started":"2025-04-27T13:35:05.557744Z","shell.execute_reply":"2025-04-27T13:35:05.562799Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/chatbot-finetuning\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python create_model.py \\\n  --model_name \"distilbert/distilgpt2\" \\\n  --pytorch_dump_folder_path \"distilgpt2\" \\\n  --model_type causal_lm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:35:21.805271Z","iopub.execute_input":"2025-04-27T13:35:21.805912Z","iopub.status.idle":"2025-04-27T13:35:50.339121Z","shell.execute_reply.started":"2025-04-27T13:35:21.805884Z","shell.execute_reply":"2025-04-27T13:35:50.338203Z"}},"outputs":[{"name":"stdout","text":"config.json: 100%|█████████████████████████████| 762/762 [00:00<00:00, 6.12MB/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\nModel config GPT2Config {\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n2025-04-27 13:35:30.982999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745760931.141870      88 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745760931.186736      88 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nmodel.safetensors: 100%|██████████████████████| 353M/353M [00:01<00:00, 280MB/s]\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\nAll model checkpoint weights were used when initializing GPT2LMHeadModel.\n\nAll the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilbert/distilgpt2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\ngeneration_config.json: 100%|███████████████████| 124/124 [00:00<00:00, 860kB/s]\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\ntokenizer_config.json: 100%|██████████████████| 26.0/26.0 [00:00<00:00, 217kB/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\nModel config GPT2Config {\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nvocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 1.93MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.27MB/s]\ntokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 7.62MB/s]\nloading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\nModel config GPT2Config {\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nConfiguration saved in distilgpt2/config.json\nConfiguration saved in distilgpt2/generation_config.json\nModel weights saved in distilgpt2/model.safetensors\ntokenizer config file saved in distilgpt2/tokenizer_config.json\nSpecial tokens file saved in distilgpt2/special_tokens_map.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!accelerate launch run_chatbot_finetuning.py ./finetune_json/chatbot.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:37:26.679781Z","iopub.execute_input":"2025-04-27T13:37:26.680090Z","iopub.status.idle":"2025-04-27T13:57:12.770852Z","shell.execute_reply.started":"2025-04-27T13:37:26.680061Z","shell.execute_reply":"2025-04-27T13:57:12.769886Z"}},"outputs":[{"name":"stdout","text":"README.md: 100%|███████████████████████████| 1.06k/1.06k [00:00<00:00, 9.05MB/s]\n(…)-00000-of-00004-2d5a1467fff1081b.parquet: 100%|█| 249M/249M [00:01<00:00, 134\n(…)-00001-of-00004-5852b56a2bd28fd9.parquet: 100%|█| 248M/248M [00:01<00:00, 148\n(…)-00002-of-00004-a26307300439e943.parquet: 100%|█| 246M/246M [00:01<00:00, 144\n(…)-00003-of-00004-d243063613e5a057.parquet: 100%|█| 248M/248M [00:01<00:00, 147\n(…)-00000-of-00001-869c898b519ad725.parquet: 100%|█| 9.99M/9.99M [00:00<00:00, 1\nGenerating train split: 100%|█| 2119719/2119719 [00:05<00:00, 370279.56 examples\nGenerating validation split: 100%|█| 21990/21990 [00:00<00:00, 409326.60 example\n[INFO|configuration_utils.py:691] 2025-04-27 13:38:00,686 >> loading configuration file distilgpt2/config.json\n[INFO|configuration_utils.py:765] 2025-04-27 13:38:00,690 >> Model config GPT2Config {\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file vocab.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file merges.txt\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2058] 2025-04-27 13:38:00,699 >> loading file chat_template.jinja\nPreprocess train dataset (num_proc=4): 100%|█| 2119719/2119719 [08:21<00:00, 422\nPreprocess train dataset (num_proc=4): 100%|█| 21990/21990 [00:05<00:00, 3977.29\n2025-04-27 13:46:29.403023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745761589.424923     130 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745761589.431631     130 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[INFO|modeling_utils.py:1121] 2025-04-27 13:46:31,871 >> loading weights file distilgpt2/model.safetensors\n[INFO|configuration_utils.py:1142] 2025-04-27 13:46:31,874 >> Generate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\n[INFO|modeling_utils.py:4930] 2025-04-27 13:46:31,929 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n\n[INFO|modeling_utils.py:4938] 2025-04-27 13:46:31,930 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n[INFO|configuration_utils.py:1095] 2025-04-27 13:46:31,931 >> loading configuration file distilgpt2/generation_config.json\n[INFO|configuration_utils.py:1142] 2025-04-27 13:46:31,931 >> Generate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\n[INFO|tokenization_utils_base.py:2510] 2025-04-27 13:46:31,932 >> tokenizer config file saved in ./test_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-27 13:46:31,932 >> Special tokens file saved in ./test_output/special_tokens_map.json\n[INFO|configuration_utils.py:419] 2025-04-27 13:46:31,991 >> Configuration saved in ./test_output/config.json\nSteps:   0%|                                             | 0/20 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-04-27 13:46:32,773 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n[WARNING|logging.py:328] 2025-04-27 13:46:33,165 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nSteps: 100%|███████████| 20/20 [06:34<00:00, 10.50s/it, lr=5e-6, step_loss=2.79][INFO|configuration_utils.py:419] 2025-04-27 13:57:08,991 >> Configuration saved in ./test_output/config.json\n[INFO|configuration_utils.py:911] 2025-04-27 13:57:08,991 >> Configuration saved in ./test_output/generation_config.json\n[INFO|modeling_utils.py:3572] 2025-04-27 13:57:09,779 >> Model weights saved in ./test_output/model.safetensors\n[INFO|tokenization_utils_base.py:2510] 2025-04-27 13:57:09,780 >> tokenizer config file saved in ./test_output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2519] 2025-04-27 13:57:09,780 >> Special tokens file saved in ./test_output/special_tokens_map.json\nSteps: 100%|███████████| 20/20 [10:37<00:00, 31.86s/it, lr=5e-6, step_loss=2.79]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}